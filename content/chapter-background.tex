\chapter{Background}\label{chap:background}
The title of this thesis is \textit{\myTitle}.
It brings together the domains of Process Science, Predictive Modeling and Deep Learning.
This chapter gives insights into the parts of these topics that will be used and needed throughout the thesis.
This work can be attributed to the domain of Predictive Process Monitoring, which will also be introduced.

\section{Process Science And Process Monitoring}
Process Science, as loosely defined by van der Aalst \cite{Aalst2016}, refers to the \textit{broader discipline that combines knowledge from information technology and knowledge from management sciences to improve and run operational processes}. This results in improvement approaches that tend to be driven by process models.

Central to process science is Business Process Management (BPM) which is a field aimed at improving operational business performance through the optimization of business processes via their models \cite{panagacos2012ultimate}.
Modeled e.g. with the Business Process Modeling Notation (BPMN) \cite{bpmn2.0}, these models are typically very rigid and permit little variation of the workflow. The central element of these notations are the individual steps of which a process is comprised, referred to as \textit{activities}, shown in \ref{fig:activity-introduction}. A running process is referred to as a case.

\begin{figure}
    \centering
    \includegraphics[width=.75\textwidth]{gfx/activity-introduction.png}
    \caption{The yellow boxes and their description are used in BPMN to describe an activity. The arrows denote the control flow in between.}
    \label{fig:activity-introduction}
\end{figure}

With the help of workflow management systems (WFMS), it is possible to embed and enforce these structured processes in an organization, making them traceable via the logs that these systems generate. The existence of these process logs is key for any analysis and thus they are also indispensable for this thesis. The execution history of a process instance is sometimes referred to as trace.

Some processes do not work well together with such a rigid world view, however. Especially in the insurance and health-care sector, each instance tends to be very different from the next \cite{hewelt2016}. This led to the notion that the course of a case strongly depends on the information contained inside it.

\subsection{Adaptive Case Management}
In settings in which it is common to employ BPM, such as assembly line productions, there is little heterogeneity. As the latter increases, BPMN and other business process modeling notations result in complex and hardly understandable models and thus fail their purpose of being easily understandable. As previously mentioned, this tends to occur in domains where the case trajectory is highly dependant on information contained inside it.

The fact that this type of process is data-driven led to the creation of the \textit{case folder} - a directory central to a case, describing its current state. In Adaptive Case Management (ACM), the process is driven through the data contained inside this folder. The person driving the case forward is referred toas case worker \cite{drucker1999}, who not only works with the information in the case but also the knowledge that he or she possesses. For this reason, case workers are often referred to as knowledge workers.

In contrast to BPM, the formalization of Case Management and the subsequent creation of modeling languages has only just begun. Notable developments include the Case Management Modeling Notation \cite{web:cmmn} as well as the Chimera approach \cite{hewelt2016}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.8\textwidth]{gfx/process-data-science.png}
    \caption{Process Mining can be understood as the bridge between Process Science and Data Science \cite[p.18]{Aalst2016}. The blue line symbolizes the subdomains that Predictive Process Monitoring brings together.}
    \label{fig:process-data-science}
\end{figure}

\subsection{Process Mining}
The logs generated by the WFMS actions give rise to the discipline of Process Mining, which covers the three steps of model discovery, conformance checking and model enhancement \cite{Aalst2016}. These steps are enabled through the use of techniques from the area of Data Science, making it possible to understand Process Mining as the link between Data Science and Process Science as in \autoref{fig:process-data-science}. An exemplary log suited for analysis is depicted in figure \ref{fig:process-log}.

A major problem in this domain is posed by \textit{spaghetti} and \textit{lasagna} models. Such overly complex and unreadable models are mined from process logs that contain highly variable execution traces. These traces often come from ACM executions.

As Process Mining is concerned with data-driven process model discovery and optimization only on offline data, a possibility to act on case developments in real-time is desirable.

\subsection{Predictive Process Monitoring}
A recent domain aiming at the use of online data to make statements about the progression of a running case is that of \textit{Predictive Process Monitoring}~\cite{francescomarino2015, schoenig2018}. The application of predictive analytics on running and thus incomplete case logs connects two subdomains of Process Science and Data Science, setting it apart from Process Mining in \autoref{fig:process-data-science}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/process-log.png}
    \caption{The first entries of an exemplary event log from a case from the BPIC 2011 dataset~\cite{BPIC2011}. Each event is associated with data; column \texttt{concept:name} corresponds to the activity title.}
    \label{fig:process-log}
\end{figure}

Instead of revolving around models, this discipline is concerned with predicting characteristics of running cases. This allows answering questions such as:

\begin{enumerate}
    \item Will I still meet my service level agreement?
    \item Will we be able to deliver the package in within our 3-hour target?
    \item How long is this case still going to take?
    \item What is going to be the next step in the case?
\end{enumerate}

The answers to these questions can give case workers the opportunity to intervene if a case takes an unwanted course or might fail to meet KPI requirements. Furthermore, this approach only requires sufficient amounts of historical case executions, but no model. While it can certainly be a useful addition, it is not needed. Especially the last point stands in strong connection to ACM, since a process model previously dictated the possible choices for the next activity.

%\begin{figure}
%	\centering
%	\includegraphics[width=\textwidth]{gfx/acm-reasoning}
%	\caption{https://acmisis.wordpress.com/what-is-adaptive-case-management-acm/}
%	\label{fig:why-acm}
%\end{figure}

\section{Predictive Modeling}
Predictive modeling is a process that uses learned statistical properties of data to predict outcomes. The models used to arrive at these outcomes shall be referred to as predictive models, unrelated to process models. This section shall introduce a common process for predictive model generation and frame a problem that is fundamental to the thesis: that of next-element predictions in a sequence.

\subsection{Predictive model development}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{gfx/kdd_process.png}
	\caption{The process for \textit{Knowledge Discovery in Databases} as per Fayyad et al. \cite{fayyad1996data}.}
	\label{fig:kdd_process}
\end{figure}

In 1996 Fayyad et al. published what came to be known as the Knowledge Discovery In Databases (KDD) process \cite{fayyad1996data}. Published 20 years ago and aimed at data mining, it is also applicable to predictive model building. It is illustrated in \autoref{fig:kdd_process}, and shall be explained step by step in the following with technical details relevant for this thesis. The process is highly iterative and jumps between every step are possible.

\subsubsection*{Selection}
Predictive models work on \textit{line items} comprised of a number of \textit{features}, which are variables that are likely to influence the prediction of the \textit{target variable}. In \autoref{fig:process-log}, each row is a line item and each column corresponds to a single feature. Features are sometimes also referred to as predictor variables.

During the selection step features are chosen as predictors based on differed criteria such as inter-variable correlations.

\subsubsection*{Pre-processing}
Some features may be sparse or contain data of low-quality. With methods from the area of data cleansing, these issues are resolved and appropriate values imputed for missing features. As we use already pre-processed data from BPIC \cite{BPIC2011, BPIC2012, BPIC2017}, this step is mentioned for the sake of completeness only.

\subsubsection*{Transformation}
\label{sec:predictive-model-development:transformation}
During transformation, some features might be aggregated, normalized or differently encoded to assist the model in picking up relations between variables. Common tasks are one-hot or dictionary encoding of discrete values such as strings, e.g. activity names.

Variable concatenations or complex aggregations might also be added as separate \textit{engineered} features \cite{schoenig2018}.

\subsubsection*{Data Mining}
The goal of predicting the next activity is a classification task. For this type of task, a wide array of applicable predictive models is available, such as linear regressions, decision trees, random forests, scalable vector machines (SVM) or artificial neural networks (ANN).

The data prepared in the previous steps is used to \textit{train} the model. During this training phase, the model uses accuracy metrics to assess the quality of its predictions, learn the statistical properties of the data and adjust its internals accordingly. Certain input parameters of the model are adjusted during this phase as well, an activity referred to as \textit{hyper-parameter tuning}. Training the model can be a very time-consuming task, unless appropriate hardware is used. Such hardware might be specially designed for model learning (i.e. Google's Tensor Processing Units) or a simple Graphics Processing Unit (GPU).

\subsubsection*{Evaluation}
Typically, not all of the available data is used for training. Instead, it is split into a \textit{training} and a \textit{test} set. The former is used for model training, while the latter is used in this step.

The predictions for the test set are compared to the known values and used to assess model performance on unseen data.

\subsection{Sequence prediction}
In this section, a formal definition of sequences is presented, and then set in relation with the problem at hand\footnote{Definition adapted from \cite{pei2001prefixspan}}.  Let $I = \{i_1, i_2, ..., i_n\}$ be the set of all items. An \textit{itemset} is a subset of $I$. A \textit{sequence} is an ordered list of itemsets, such that in its notation $seq = \langle s_1s_2...s_l \rangle\ |\ \forall\ 0 \leq j \leq l: s_j \subseteq I$. Then, $S$ defines the infinite set of all possible sequences. It is infinite because sequences can be arbitrarily long, with each itemset containing an arbitrary number of items. For  subsequences, the notation $seq_{i,k}$ denotes the containment of the elements from index $i$ through $k$ from the original sequence: $seq_{1,2} = \langle s_1s_2 \rangle$.

Assuming that a a sequence can be finished, consider the database of finished sequences $DS$. Under the assumption of the Markovian hypothesis that "the probability of each event depends only on the state attained in the previous event"\cite{gagniuc2017markov}, a predictive model can be trained on this database to predict the next sequence $nseq$ or itemset $s_k$ of an incomplete sequence $seq_{inc}$:

\begin{equation}
\begin{split}
    predict(seq_{inc}) &= \widehat{nseq}\\
    predict(seq_{inc}) &= \hat{s_k}\ |\ 0 \leq k \leq l
\end{split}
\label{eq:prediction-from-sequence}
\end{equation}

Sequence predictions are a common problem in the domains of machine translation and text generation. For example, the translation of the sentence \textit{"I am writing my master's thesis"} into the german sentence \textit{"Ich schreibe meine Masterarbeit"} can easily be mapped onto the notation previously described. With $I$ being the set of alphabet and punctuation marks and each itemset representing a single word, the input and target sequences could be noted as:

\begin{equation*}
\begin{split}
seq_{in} &= \langle<I>, < >, <am>, ... <thesis>\rangle\\
seq_{out} &= \langle<Ich> ... <Masterarbeit>\rangle
\end{split}
\end{equation*}

Referring to \autoref{eq:prediction-from-sequence}, $seq_{in}$ can be understood as the argument to $predict$ and $seq_{out}$ as $\widehat{nseq}$. This is a simple example of a \textit{sequence-to-sequence} prediction.

Next to this kind of prediction, there are also \textit{sequence-to-word} predictions, which can be used to generate text and even write simple novels~\cite{web:text-generation-machinelearningmastery, web:text-generation-freecodecamp}. The transfer of this general formulation onto the translation problem is easy to imagine, since the prediction target of a sequence need not be a word of an incomplete sentence - it could also be the name of an activity of an ongoing business process. The transfer of the formulation of sequence prediction to business processes shall be presented in \autoref{sec:contribution}.

\section{Artificial Neural Networks}
An artificial neural network (ANN) mimics the inner workings of a human brain in that it is made up of connected nodes referred to as neurons. These neurons are interconnected and act on the incoming signals from each connection. This section gives background on the architectural setup of a classic neural network and two of its enhancements, as both are integral parts of this thesis.

\subsection{Feedforward neural networks}
In typical ANN implementations, the signal at an edge between neurons is a number, and the output of each artificial neuron is computed by a function of the sum of its inputs. This function is called \textit{activation function}, as it decides whether the neuron emits a signal or not. Edges are weighted with the weights being adjusted as the learning proceeds. During learning, data is passed from layer to layer in synchronized steps. The passing of all training data through the network once is called an \textit{epoch}.

Furthermore, the nodes are organized in layers, with each neuron connecting to every neuron in the adjacent layers resulting in fully connected layers. The layers between the input and output layers are referred to as \textit{hidden layers} and  can be greater in number. \autoref{fig:feedforward-ann} showcases a simple ANN architecture and illustrates the inner workings of a neuron.

\begin{figure}
    \centering
    \includegraphics[width=.85\textwidth]{gfx/feedforward-neural-network.png}
    \caption{A simple feed-forward ANN architecture with a single hidden layer. The networks takes the inputs $x_1$ to $x_m$ and passes them through edges that are weighted with weights $\beta_{ij}$. The illustration is taken from \cite{lessmannBADS}.}
    \label{fig:feedforward-ann}
\end{figure}

Classical ANNs are moving their internal signals in a single direction: toward the output layer. This behaviour is called feedforward and causes the network not to have any capacity for persisting what it has previously processed. Put simply, it is not able to \textit{remember}.

Recurrent neural networks have been created with this capacity in mind, long short-term memory enhancing it further.

\subsection{Recurrent neural networks}
As the name suggests, recurrent neural networks (RNN) implement a feedback loop. This loop makes information from the output of a neuron available to that same neuron for the next step. In consideration of the time dimension, RNNs are often displayed in an unrolled fashion as in \autoref{fig:rnn-unrolled}. This form of illustration also reveals that recurrent neural networks are intimately related to sequences and lists.

\begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{gfx/rnn-unrolled.png}
    \caption{A neuron receives an input $x_t$ and outputs a value $h_t$. The loop allows information to be passed from one step $t$ of the network to the next step $t+1$. This Illustration is taken from \cite{web:colah}.}
    \label{fig:rnn-unrolled}
\end{figure}

While the loop indeed allows recognizing short-term dependencies, the network as a whole still struggles with long-term dependencies when the gaps between related inputs simply become too great. This problem has been thoroughly explored by Hochreiter et al.~\cite{hochreiter1991untersuchungen} who also proposed a fix with so-called long short-term memory.

\subsection{Long short-term memory}
In recent years, RNNs were applied with great success to a variety of problems: speech recognition, language modeling or translation. This success can be attributed in part to the enhancement of RNNs with long short-term memory (LSTM) cells.

Hochreiter \& Schmidhuber published this enhancement in 1997 \cite{hochreiter1997} which now sees wide application. Essentially, the repeating module of a RNN was equipped with a state, i.e. the capacity to remember. The cell state $C$ is managed through chaining together different operators, one of them also representing a capacity to forget. \autoref{fig:lstm} showcases an unwrapped LSTM layer with its internal architecture exposed. Variants of the original LSTM cell architecture exist, with most modifications made on the construction of the state $C$ - all exhibiting very similar performance \cite{greff2017lstm}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.8\textwidth]{gfx/lstm-chain.png}
    \caption{An exemplary unrolled LSTM repeating module with the upper horizontal line representing the cell state $C$. This Illustration is taken from \cite{web:colah}.}
    \label{fig:lstm}
\end{figure}

\section{Data preparation and feature engineering}
To assist predictive models during training, most features need to be reformatted. This section shall highlight the relevant methods used in this thesis by illustrating how a feature set using this encoding would be constructed from a sequence.

\subsection{Feature engineering for categorical features}
While ordinal features are most often normalized, categorical features can be encoded in a variety of ways. Where one-hot and dictionary encoding are traditional approaches, word embeddings represent a relatively new and promising approach:

\subsubsection*{One-hot encoding}
One-hot encoding is often used for features which have a small to medium-sized number of unique values. Using it, each line item of a feature is expanded to an $n$ column representation, where each $n$ represents the number a unique values of the original. The columns hold a boolean flag $b$, of which only one is true in any given row, hence the name. This type of encoding is also sometimes referred to as "dummy encoding".

The following equation shows the feature set construction for a sequence $seq$ of one-item itemsets:
\begin{equation}\label{eq:one-hot}
    one\_hot(seq) = \{(b_1, ..., b_n) | \forall s \in seq: \wedge b_i = 
    \begin{cases}
    1, & \text{if index corresponds to value}.\\
    0, & \text{otherwise}.
    \end{cases}
    \}
\end{equation}
\todo[inline]{The way this looks there is still potential}

\subsubsection*{Dictionary encoding}
Widely used in compression, for example in in-memory database systems~\cite{plattner2012memory}, dictionary encoding is also used to encode categorical values. Since predictive models only process numerical values, dictionary encoding is used to create a look-up table and map every value of a feature to a numeric code.

In contrast to one-hot encoding, this dictionary encoding does not result in wide and sparse inputs, making it suitable for features with a large number of distinct features. However, it has one important ramification: It imposes an order on features which previously might not have had an one. If e.g. the word "car" is encoded with a 1 and the word "red" is encoded with a 2, the following expression suddenly holds true: "car" $<$ "red".

\subsubsection*{Word embedding}
Because dictionary encodings impose an order, and one-hot encodings lead to sparse and wide inputs, word embeddings are perceived as a viable alternative for encoding categorical variables.

A word embedding is the output of a neural network with a single hidden layer that has been trained on a large text corpus and thus detected how words relay to each other \cite{web:word-embedding}. The network puts out highly dimensional vectors for each word which represent any word property that the model has learned by. This allows for interesting examples with vector arithmetic, such as the one shown in \autoref{eq:word-embedding:king}. In this famous example the network was able to detect gender properties and social semantics of a word. 

\begin{equation}
    \label{eq:word-embedding:king}
    \vec{v}_{king} - \vec{v}_{man} + \vec{v}_{women} = \vec{v}_{queen}
\end{equation}

A word embedding thus allows clustering of words by certain properties.
Made popular by Google, the most extensive implementation for word embeddings is word2vec \cite{web:ahogrammer, goldberg2014word2vec}.
\todo[inline]{Refer to embeddings in related work when presenting Francescomarinos work}

\subsection{Feature engineering for variable-length sequences}
Predictive models take inputs of fixed size. As shown, sequences can be of arbitrary length, and thus the curse of dimensionality also appears in context with this problem. To bring variable length inputs into a usable format for predictive models, several methods of formatting have been invented. As some of their names names suggest, these come from the domain of Natural Language Processing (NLP), but can easily be transferred to the problem at hand. These shall be introduced in the following along with information on how each method can be use to construct a single line item.

\subsubsection*{Sliding Window}
The sliding window format is very common in the areas of NLP and Time Series Forecasting. By using it, chunks of length $c$ of the sequence $seq$ are fed into the prediction model. This results in several input tuples for every sequence, which can be many depending on $c$. \autoref{eq:sliding-window:tuple-set} shows how a sequence results in a set of tuples:

\begin{equation}
    \label{eq:sliding-window:tuple-set}
    \{ (s_k, s_{k+1}, ..., s_{k+c})\ |\ 1 \leq k \leq l-c \wedge s_k \in seq \}
\end{equation}

\subsubsection*{Bag-Of-Words}
The bag-of-words (BOW) approach produces $l$ input tuples, the length of sequence $seq$. Similarly to one-hot encoding, the possibly occuring values need to be known. One arrives at the encoding by counting the occurrences of each item or itemset in $seq$ for each subsequence $seq_{1,k}$. \autoref{eq:bow:multiset} showcases the multiset containing one line item for each further step into the sequence:

\begin{equation}
    \label{eq:bow:multiset}
    \{ bow\_ss(seq_{1,k})\ |\ 1 \leq k \leq c \} \\
    bow\_ss(seq) = ( s_1^{count(s_1,seq)}, ..., s_n^{count(s_n,seq)} )
\end{equation}
\todo[inline]{Optical enhancement}

\subsubsection*{$n$-grams}
The $n$-gram approach is very popular in computational linguistics, biology and data compression and is effectively an $(n-1)$-order Markov model, with the most popular choices for $n$ being $1,2$ and $3$. These models would be called \textit{unigram}, \textit{bigram} and \textit{trigram} models.

Similar to BOW, an $n$-gram model counts frequencies. Different from to BOW however, it counts the occurences of subsequences of length $n$. Suppose a set $LS$ holds all possible values of a single feature. From $LS$, a feature set $FS$ is constructed with every item being a permutation of length $n$. A feature in this set would be referred to as \textit{gram}, as \autoref{eq:ngram-fs-construction} shows.

\begin{equation}\label{eq:ngram-fs-construction}
%    FS = \{LS_{i,i+n}\ |\ 1 \leq i \leq |LS|-n \}
    FS = S(LS, n)
\end{equation}

In a line item, each $FS$ element would be represented by a number $f_i$, denoting whether this specific subsequence already occurred within a given sequence. \autoref{eq:ngram-tuple} formalizes the construction of a single $n$-gram-encoded tuple of sequence $seq$ at step $k$:

\begin{equation}\label{eq:ngram-tuple}
    ngram\_tuple(seq_{1,k}) = (f_1, ..., f_{|FS|})\ |\ \ f_i = 
    \begin{cases}
    1, & \text{if $f_i \in seq_{1,k}$}.\\
    0, & \text{otherwise}.
    \end{cases}
\end{equation}

While $n$-grams can be be powerful features to use, their high dimensionality causes high computational loads. It helps find words that occur together and possibly have a meaning together. The higher the $n$, the more detailed these findings are, leading to the next approach of dealing with sequences.

\subsubsection*{Subsequence mining}
One of the main weaknesses of $n$-grams is the restriction to a single $n$ and that "one may need to examine a combinatorially explosive
number of possible subsequence patterns" \cite{pei2001prefixspan}. For this reason, algorithms have been developed that combine the benefit of $n$-grams with more flexibility by avoiding the limitation to $n$. These algorithms sift through input sequences and detect sequential patterns of any length, resulting in line items similar to the ones in \autoref{eq:ngram-tuple} with the $f_i$ denoting the presence of a specific subsequence in the sequence instead of ngrams.

Examples for these algorithms are GSP \cite{srikant1996gsp}, FreeSpan \cite{han2000freespan} and PrefixSpan \cite{pei2001prefixspan}.