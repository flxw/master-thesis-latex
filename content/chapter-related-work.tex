\chapter{Related Work}\label{chap:related-work}
As this thesis brings together Predictive Process Monitoring with an approach from NLP, this chapter on related literature presents selected publications from both domains.

Publications related to Predictive Process Monitoring are presented in \autoref{sec:related-work-predictive-process-monitoring}, with the chapter ending in a detailed presentation of the two publications reimplemented for comparison. Similarly, \autoref{sec:related-work-sequence-prediction} presents other sequence prediction approaches from NLP and ends with in-depth information on a publication which we adapted for the use case at hand.

\section{Predictive Process Monitoring}\label{sec:related-work-predictive-process-monitoring}
We found that there are three types of prediction targets in current publications on Predictive Process Monitoring: constraint predictions, case-specific next-event predictions, and general next-event predictions without regard to a specific case. These three types will serve as subdivisions to this section.

\subsection*{Constraint predictions}
A constraint could be the delivery of a product within a given timeframe, or the total runtime of a case~\cite{weske2012business, francescomarino2015}.

Polato et al. make use of data attributes attached to events in their work for improving the prediction of the remaining time of business process instances~\cite{polato2014}. During feature engineering, their training data is enriched with information about possible other activities. With support vector regression (SVR) from the WEKA toolkit~\cite{web:weka} and default parameters, they are able to reach $6\%$ and $9\%$ mean absolute percentage error (MAPE) on two non-public datasets of 5000 and 1500 traces. While the authors criticize the use of non-public datasets, they also do not publish theirs nor their source code, making the results hard to compare.\\

Metzger et al. predict constraint violation of a case by comparing fundamentally different prediction models and combining them into an ensemble. The ensemble is tested with different voting mechanisms, e.g. majority voting or recall-orientation. They argue that late and precise or early and incorrect predictions are equally worthless for interventions, and that predictions should stabilize as early as possible. They establish that after progressing $60\%$ to $85\%$ into the case, predictions become stable and precise.

Three approaches are used to predict constraint violations: constraint satisfaction, Quality-of-Service (QoS) violation checks and machine learning. The two former models use rules to predict a process outcome while the latter is a trained neural network. For it, Metzger et al. use the ANN implementation from the WEKA machine learning toolkit with a single hidden layer.

The authors are able to formulate rules for the two other models because the process at hand is very rigid and well defined: The Cargo 2000 process is a standard proposed by the International Air Transport Association (IATA)~\cite{metzger2015}. Two thirds of their non-public dataset of 3942 traces and 56082 events were used for training, and the remainder for testing.

While model ensembles can be expected to bring small accuracy improvements, they incur a lot of work and are typically only found in research~\cite{lessmannBADS}. Furthermore, we expect that the use a RNN on this simple processs would have yielded a high accuracy than the stated $0.7$.\\

Francescomarino et al. organized predictive models in a clustered fashion~\cite{francescomarino2015} to predict predicate fulfilment. Having clustered the training data, one model was trained on each cluster. Then, to obtain a prediction, the cluster for a new data item needed to be found, and the corresponding model selected. Furthermore, the authors varied the probability threshold for accepting a prediction and measured how it affected the point in process progress at which the predictions become stable (similar to Metzger et al.). They refer to this characteristic as \textit{earliness}. Their approach, implemented as a ProM plugin called \textit{Predictive Process Monitoring}, uses k-means or DBSCAN to cluster the data and decision trees or random forests to make the prediction. It was tested on the BPIC11~\cite{BPIC2011} dataset, and obtained accuracies of up to $0.9$.

Clustering improves tree-based models because it makes the training data more homogeneous. By using embedding layers, an ANN can cluster the data by itself, making a manual clustering unneccessary.

\subsection*{Case-specific next-event predictions}
Next-event predictions specific to a case allow answering the question "which activity comes next in this special case?" - the question that this thesis tries to answer with deep learning.\\

Huber~\cite{huber2015} gives an example for how a next-step prediction and recommendation system for case workers might look like. The system is prototypically implemented within CoCaMa, a case management application. Its predictions are produced as follows: After gathering the training data from various sources via an extract, transform and load (ETL) process, several \textit{Next Models} are constructed. There are four Next Models which use different data: timestamps, deadlines, decisions, and goals. The predictions from these individual models are combined by the recommender via weightsto produce a recommendation. Huber uses decision trees from the WEKA~\cite{web:weka} machine learning toolkit to implement them. The system has been evaluated with 25 hand-made traces~\cite{huber2015}, which explains why high accuracies could not be obtained.\\

Next-event prediction without any machine learning is done by Böhmer et al. with a method rooted in heuristic analysis~\cite{boehmer2018probability}. The authors argue that the amount of trust that users put into ANN predictions is limited due to the fact that ANNs are not only computationally expensive, but that their black-box nature makes their predictions and inner workings very hard to comprehend. As potentially large organizational changes could be made based on the predictions, the authors perceive it of importance to explain alternative futures which were not classified as being most probable and explain the aspects which motivated specific prediction results.
Böhmer et al. approximate trace similarities with the Damerau-Levenstein method and a custom cost function. Using this method, they filter historic traces to produce a set of similar executions, and go on to mine probability distributions from it. The most probable behaviour is used as the final next-event prediction~\cite{boehmer2018probability}. Additionally, they predict the timestamp of its execution. They evaluate their approach on the BPIC12 datase~\cite{BPIC2012} as well as a Helpdesk event log~\cite{Helpdesk}. On both dataset, they achieve an accuracy of $0.77$ for the next-event predictions.

While understanding the reasons for a prediction is important, forgoing black-box models completely could mean missing out on potentially higher accuracies~\cite{tax2018interdisciplinary}. With techniques such as LIME, predictions made by black-box models can be explained~\cite{ribeiro2016should}, albeit the computational power required is undeniably higher.\\

Klinkmüller et al. enrich their training dataset with features that encode subsequence occurrence~\cite{klinkmuller2018reliablemonitoring} to predict the next event in a trace based on its history, which they refer to as prefix. Doing so on a synthetic dataset, they are able to increase the accuracy of a random forest in comparison to a baseline that is not using the engineered subsequence features. This approach is similar to that of Shibata et al.~\cite{shibata2016bipartite}, which is thoroughly examined in the next section. Furthermore, the authors find that training models on complete traces is preferable to the popular practice of trace truncation.\\

Francescomarino et al. also used LSTM networks to predict the next events of a case, and engineered features which indicate the presence of a loop~\cite{francescomarino2017}. To enhance the predictions further, they apply a compliance-checking logic on top of the predictions which leverages a-priori knowledge to rule out forbidden next events. The networks is trained with windowed samples, and the following accuracies on these six datasets are obtained:
EnvLog~\cite{EnvLog}: $0.07$, HelpDesk~\cite{Helpdesk}: $0.816$, BPIC11~\cite{BPIC2011}: $0.276$, BPIC12~\cite{BPIC2012}: $0.408$, BPIC13~\cite{BPIC2013}: $0.516$, BPIC17~\cite{BPIC2017}: $0.439$

\subsection*{General next-event predictions}
General next-event predictions predict the next event in a stream of events, without specifying which case this event belongs to. Here, Evermann et al.~\cite{evermann2016} and Schönig et al.~\cite{schoenig2018} made contributions, which also build upon each other.

Evermann et al. remark the lack of research on next event predictions and have successfully demonstrated the applicability of LSTM neural networks in the context of predicting the next event in a stream of events. Using a neural network implemented with Tensorflow, precisions between $0.60$ to $0.90$ on the BPIC2012 and BPIC2013 datasets are achieved. Evermann et al. want their work to be understood as a "demonstration of the applicability of the approach and the potential for future work". The authors highlight that their work lacks use of data attributes during model training.

Schönig et al.~\cite{schoenig2018} picked up on the last point and demonstrated on the BPIC dataset from 2017 that using data attributes complementary to the event names does increase prediction accuracy. Schönig et al. implemented their solution with Keras and trained it with stratified 5-fold cross-validation.
The data was pre-processed with one-hot encoding for categorical variables and min-max-normalization for continuous features. Also, the work demonstrates that an increasing number of included data attributes can improve accuracy~\cite[p.5]{schoenig2018}, without referring to any of their statistical properties. Their approach reaches accuracies around $0.90$, depending on its configuration. It is important to note however, that neither Evermann et al. nor Schönig et al. predict the next event \textit{specific to a trace}, but the next event in the whole event stream without attribution to a case.

\begin{figure}
\centering
\subfloat[][Architecture used by Evermann et al.~\cite{evermann2016}.]{
    \includegraphics[width=0.3\textwidth]{gfx/evermann-network-architecture.png}
    \label{fig:evermann-architecture}
}
\qquad
\subfloat[][Architecture used by Schönig et al.~\cite{schoenig2018}.]{
    \includegraphics[width=0.3\textwidth]{gfx/schoenig-network-architecture.png}
    \label{fig:schoenig-architecture}
}
\caption[Overview of the reverse-engineered networks]{An overview of the reverse-engineered networks that are used as a comparison in this thesis. $n$ denotes the dimensionality of the input vector, and $m$ the number of output classes, including the end-of-sequence marker.}
\label{fig:benchmark-architectures}
\end{figure}

We take the works of Evermann et al. and Schönig et al. and reimplement them as direct comparisons to our adapted approaches as they were tested on BPIC data, their source code was made readily available and their authors were helpful in ensuring the correctness of our understanding. Furthermore, their approaches were fully based on neural-networks with little additional pre- or postprocessing. In close collaboration with the two authors, their neural networks were reverse-engineered and also the architectures in \autoref{fig:benchmark-architectures} were confirmed to be correct. Both model the prediction task as a multi-classification problem.

Clearly, Evermann's architecture left an inspiring impression with Schönig, who decided to remove the Embedding layer and adapt unit counts and activation functions. In our conversation, Schönig argued that the Embedding was not needed, as the number of unique events was a lot lower than that of words in a text, for which Embeddings were originally developed. Where Evermann et al. used stochastic gradient descent (SGD) with a manual adaption of learning rate decay to $0.75$ after the 25th epoch, Schönig et al. use RMSprop with default values to optimize the network's loss. Furthermore, Schönig et al. fed the training data into their networks in a windowed fashion, contrary to Klinkmüller's suggestion that this might produce unstable results~\cite{klinkmuller2018reliablemonitoring}.

\section{Sequence prediction}\label{sec:related-work-sequence-prediction}
Sequence prediction deals with predicting the next element or next sequence from a given input, as explained in \autoref{sec:background:sequence-prediction}. It is rooted in NLP, and relevant publications are discussed in this section.\\

Attention is a fairly new development in the domain of ANNs, and it has been leveraged by Kokkinos et al. in a tree-structured neural network to classify sentiments in sentences~\cite{kokkinos2017structural}. In a detailed comparison with other works, their bipartite tree approach yields the highest accuracy on the Stanford Sentiment Treebank dataset. While the proposition of tree-structured Gated Recurrent Units (GRUs, a variant of LSTM units) and their use of attention forms the main contribution of this work, the authors make use of a bipartite network architecture and word embeddings as well. This gives further appeal to the idea of adapting the approach of Shibata et al, presented in the following.\\

In 2016, the International Conference in Grammatical Inference 2016 (ICGI) held a competition called SPiCE\ "about guessing the next element in a sequence"~\cite{web:spice}. It entailed making next-element predictions on twelve different datasets on which to make predictions for the next word. Most entries submitted to this competition made use of RNNs with LSTM cells to do so.\\

\begin{figure}
    \centering
    \includegraphics[height=.4\textwidth]{gfx/spice-winner-architecture.png}
    \caption{Neural network architecture of the winning submission at the SPiCE competition~\cite{shibata2016bipartite}}
    \label{fig:spice-winner-architecture}
\end{figure}

The winning submission by Shibata et al. uses a bipartite network architecture, training separate layers on different features of the same sentence. The results of these separate layers are then merged in the middle hidden layers to produce a single output~\cite{shibata2016bipartite}, as \autoref{fig:spice-winner-architecture} shows. Architectural similarities with Schönig et al. and Evermann et al. are especially prominent in the left part of Shibata's model.
While one half of the layers are trained on the most recent word of the sentence, the other half is trained on the prefix of that word. As this prefix can be of any length, Shibata et al. propose a binary bag-of-words encoding, representing the states of an SP-2 automaton.

SP-$k$ languages are used to describe certain long-term dependencies through forbidden subsequences. For example, if $\langle a,b \rangle$ is forbidden, then no $b$ may ever occur after $a$. According to Heinz, who assisted Shibata, deterministic finite automatons (DFA) can also be used to characterize SP-$k$ languages, if the DFA states encode those subsequences of size $k-1$ present in the previous prefixes~\cite{heinz2010estimatingSP}. To make this concept more tangible, \autoref{tab:sp2-encoding} illustrates a small example. The authors argue that LSTMs are not completely understood yet and it has not been proven that they are fully capable of recognizing sequences, which is why these features should assist the network. To prove their point, they compare the SP-$k$ bipartite architecture with a basic one that is nearly identical to the one by Evermann et al. in \autoref{fig:evermann-architecture}. The performance differences between the two models are acknowledged as generally "slight", while the basic architecture performs "significantly worse on three problems".\\

\begin{table}[!htb]
    \centering
    \begin{tabular}{cclccccc}
        \hline
          &      &              & \multicolumn{5}{c}{SP-2 vector}\\
        t & a(t) & prefix(a(t)) & [a & b & c & d & e]\\
        \hline
        0 & a    & a            & [1 & 0 & 0 & 0 & 0]\\
        1 & d    & ad           & [1 & 0 & 0 & 1 & 0]\\
        2 & a    & ada          & [1 & 0 & 0 & 1 & 0]\\
        3 & c    & adac         & [1 & 0 & 1 & 1 & 0]\\
        4 & d    & adacd        & [1 & 0 & 1 & 1 & 0]\\
        \hline
    \end{tabular}
    \caption[SP-2 feature vector example]{Prefixes encoded with SP-2. As $t$ progresses, more and more single-item subsequences ($k-1=1$) are marked as occurred. The alphabet is $I=\{a,b,c,d,e\}$. This example is taken from Shibata et al. ~\cite{shibata2016bipartite}.}
    \label{tab:sp2-encoding}
\end{table}
