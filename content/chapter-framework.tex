\chapter{Batch construction strategies}\label{chap:training-framework}
During the development of the models, we realized that there are highly divergent understandings regarding the construction of batches from sequential data.
We are convinced that these different understandings have a substantial impact on the reproducibility of any prediction approach.
Furthermore, we needed to automate the training of the networks because we used multiple datasets.
We present here a model training framework as a solution to both challenges.
We share it because we believe that it enables researchers to compare the different understandings and quickly try out new model architectures.
In doing so, we hope to contribute to Improvement Area 2 (Reproducibility) and Area 3 (Comparability) in \autoref{sec:intro:motivation}.

We show a selection of possible batch construction strategies in \autoref{sec:contrib:input-formatting} and the underlying technical reasons. Finally, we present the training framework in \autoref{sec:contrib:training-framework}.

\section{Contrasting different strategies}\label{sec:contrib:input-formatting}
We implemented the SP2 and PFS models as well as the comparison models using Python and the Keras neural networks API.
While doing so, a wide range of recommended possibilities to train the models on sequential data were noted in relevant publications and on online platforms.\\

Some recommend sliding window approaches, while others deem this against the concept of LSTM cells.
Especially Klinkmüller et al. argue against it: "[...]the popular strategy of cutting traces to certain prefix lengths to learn prediction models for ongoing instances is prone to yield unreliable models"~\cite{klinkmuller2018reliablemonitoring}.
Again others argue that padding and truncating sequences to the same length makes training faster and does not affect accuracy.
The batch size strongly affects convergence properties of any neural network, making it an important hyper-parameter to tune~\cite{keskar2016large}.
No guidelines how to sort traces into batches were found.
We believe that the understanding of the merits of each batch construction strategy can be of general value to any application of sequence prediction.\\

Keras' implementation of LSTM is stateful \textit{during} a batch, and resets the cell state $C$ before the next batch~\cite{web:keras}.
The state reset is configurable, but for the sake of simplicity, it is advisable to keep related timesteps inside a single batch.
Translated to traces, this means keeping a trace completely inside a batch.
A Keras LSTM layer requires input data to be formatted in a three-dimensional array of fixed size for every batch.
Each dimension has a defined meaning as indicated by the suffix:

$$(n_{samples}, n_{timesteps}, n_{features})$$

A batch contains $n_{samples}$ samples, which correspond to traces in our case.
Every sample contains $n_{timesteps}$ feature vectors.
Each of these vectors has $n_{features}$ dimensions.
These dimensions correspond to the size of the feature vector that is fed into the input layer.
For each sample, the Keras LSTM layer cells maintain a separate state.
This means that several traces can be trained on simultaneously~\cite{web:keras-lstm-state}.
As the batches themselves do not need to have the same dimensions, this definition opens up a range of strategies to construct batches from traces of variable length.
We highlight four of them in the following paragraphs.
To the best of our knowledge, these have not been compared in this context yet.\\

\paragraph{The individual strategy}
First and easiest, batches can be constructed from a single sample.
This fulfills the requirement of constant dimensions since only a single sample is used.
\autoref{fig:individual-batching-example} illustrates this batching strategy.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/individual-batching.pdf}
    \caption[Individual strategy for batches]{Batch construction from a single sample. A batch is boxed in a blue rectangle, and its dimensions are written beside it.}
    \label{fig:individual-batching-example}
\end{figure}

\paragraph{The grouping strategy}
Second, \textit{some} traces can be trained inside a single batch.
This is possible if all traces inside a batch have the same number of timesteps, i.e., have the same length.
\autoref{fig:grouped-batching-example} illustrates how a grouping could look like.
The figure also illustrates that the number of samples and timesteps may vary between batches.
A look at \autoref{fig:bpic2011-length-distribution} reveals a power-law distribution of trace lengths in BPIC11.
The grouping strategy could bias the model toward the long tail of the distribution, as longer and fewer traces are put into batches of their own.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/grouped-batching.pdf}
    \caption[Grouping strategy for batches]{Batch construction by grouping samples. A batch is boxed in a blue rectangle, and its dimensions are written beside it.}
    \label{fig:grouped-batching-example}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic11-length-frequency.pdf}
    \caption[Distribution of trace lengths in BPIC11]{Distribution of trace lengths in BPIC11. Logarithmic scale to highlight extremely long outliers.}
    \label{fig:bpic2011-length-distribution}
\end{figure}

\paragraph{The padding strategy}
 there is the possibility of padding the number of timesteps in a sequence to the same length.
The padded values are then filtered out using a Masking layer during training~\cite{web:keras}.
The padding length is determined by the maximum trace length and may incur a significant memory overhead.
This is especially the case for large outlier values like in BPIC11.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=.9\textwidth]{gfx/padded-batching.pdf}
    \caption[Padding strategy for batches]{Batch construction from padded samples. Padding values are depicted by gray boxes.}
    \label{fig:padded-batching-example}
\end{figure}

\paragraph{The windowing strategy}
Fourth and finally, there is also the possibility to split the trace into samples by sliding a window along it.
This results in $l-w+1$ samples for a trace of length $l$ and a window width $w$.
Taking \autoref{fig:windowing-strategy-example} as an example, the window would be $w=2$ timesteps wide.
This approach solves the problem of unequal sample lengths and facilitates batch construction.
Unfortunately, it only allows the model to use a maximum of $w$ timesteps per sample for training.
This might cause the model to miss potential long-term dependencies.
As both Evermann et al.~\cite{evermann2016} and Schönig et al.~\cite{schoenig2018} use this format, and it directly opposes the findings of Klinkmüller et al.~\cite{klinkmuller2018reliablemonitoring}, we investigate it.
Furthermore, we are convinced that use of a windowed training data format misses out on LSTM potential.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=.55\textwidth,angle=-90,origin=c]{gfx/windowing-strategy.pdf}
    \caption[Windowing strategy for batches]{Batch construction by sliding a window along a sequence $\langle A,C,C,B\rangle$. A batch is boxed in a blue rectangle, and its dimensions are written beside it.}
    \label{fig:windowing-strategy-example}
\end{figure}

\section{A training framework for sequential data}
\label{sec:contrib:training-framework}
We realize that making different process prediction approaches comparable is not only a technical but also a data-related challenge.
No established benchmarking datasets for predictive process monitoring exist yet, which complicates comparison.
As a result, more and more datasets are used, and demand more and more work to establish comparisons.
To mitigate this situation, we designed a software framework that helped us compare the sixteen total model-batching-strategy combinations on a variety of datasets in an extensible way.
We want to make this framework accessible to future researchers to facilitate model development and sharing of implementations.\\

\begin{figure}
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=1.25\textwidth]{gfx/training-framework-classes.pdf}
    \end{adjustbox}
    \caption[UML diagram of the framework classes]{Unified Modeling Language (UML) diagram of the abstract base class inheritance structure of Builders and Batchers}
    \label{fig:trainig-framework-classes}
\end{figure}

The proposed framework provides a simple training frontend and integrates two concepts: Builders and Batchers. As \autoref{fig:trainig-framework-classes} shows, the two concepts are introduced as abstract base classes from which the actual model and batching strategy implementations are derived.\\

\noindent\textbf{Builders} construct Keras models and also define the structure of the training and test data. This is important as models may not only have one input layer but two or more. These pre-formatted data sets are not structured into batches yet. One such Builder is implemented for every model type by inheriting from the abstract base class \verb=AbstractBuilder=.\\

\noindent\textbf{Batchers} take the pre-formatted data sets and structure them into batches. For each batching strategy, a class inherits from the abstract base class \verb=AbstractBuilder=.\\

\noindent The \verb=model_runner= frontend ties these two concepts together with training logic and provides a basic command-line interface as \autoref{fig:framework-frontend} shows. It allows for flexible training of a model with a specific Builder and a specific Batcher. The frontend permits placing the training process on a specific GPU and allows defining the output directory for the model and performance measurement files. The training algorithm also implements early stopping, thresholds of which can be configured via a configuration file.\\

With the help of this framework, future researchers only need to subclass \verb=AbstractBuilder=, and can directly evaluate the model performance on any given dataset without having to set up the whole training environment. By merely sharing their Builder and Batcher implementations, it will be much easier for other researchers to reproduce findings. The framework and detailed documentation is hosted under \href{https://github.com/flxw/nitro4ppm}{github.com/flxw/nitro4ppm}.\\

In this chapter, we explained how Keras requires the dimensions of samples in a batch to be constant,
and outlined four strategies to deal with this requirement.
The strategies present ways to construct batches from variable-length sequential data that honor Keras' requirement.
We also presented how the training framework integrates these strategies, and
facilitates training models with multiple strategies on multiple datasets.
In the next chapter, we explain which datasets we use to train the models,
and how each model was configured.

\begin{figure}[!htb]
\centering
\begin{verbatim}
usage: model_runner.py [-h] [--gpu GPU]
                       [--output OUTPUT]
                       {evermann,schoenig,sp2,pfs}
                       {padding,grouping,individual,windowing}
                       datapath

The network training runner script of nitro4ppm!

positional arguments:
  {evermann,schoenig,sp2,pfs}
                        Which type of model to train.
  {padding,grouping,individual,windowing}
                        Which strategy to use for feeding
                        the data into the model.
  datapath              Path of dataset to use for training.

optional arguments:
  -h, --help            show this help message and exit
  --gpu GPU             CUDA ID of which GPU the model
                        should be placed on to
  --output OUTPUT       Target directory to put model
                        and training statistics
\end{verbatim}
\caption[CLI frontend for the framework]{The \texttt{model\_runner} command-line frontend for the training framework}
\label{fig:framework-frontend}
\end{figure}
