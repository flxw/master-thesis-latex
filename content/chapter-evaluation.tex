\chapter{Evaluation}\label{chap:evaluation}
In this chapter, we present our evaluation criteria and the findings which we got from them.
We evaluate the four models with a three-pronged strategy, targeting the following characteristics:

\begin{enumerate}
    \item\textbf{Accuracy} - the share of correct next-activity predictions
    \item\textbf{Training time} - the amount of time required for training
    \item\textbf{Stability} - the change in prediction accuracy with progress
\end{enumerate}

We measure accuracy because it is a good indicator of how often the model predicts the right class, and permits comparisons to other works.
Training time is of interest because a real-world application should use as little computing resources as possible.
We forgo inference timing measurements because few predictive process monitoring solutions are deployed in application scenarios yet.
The stability criterion permits making a judgment about the behavior of accuracy over time. The works of Francescomarino et al.~\cite{francescomarino2015} and Klinkm√ºller et al.~\cite{klinkmuller2018reliablemonitoring} inspired this measure. It provides an understanding of how the prediction accuracy of a model changes as the trace grows longer. This can facilitate building trust in the model, as indicated in the introduction of the thesis~\cite{klinkmuller2018reliablemonitoring, boehmer2018probability}.
During the evaluation, results for BPIC logs and the HelpDesk dataset should not be compared too closely, as the BPIC logs are a lot more resourceful in terms of data attributes.

We discuss the acurracy of the models in \autoref{sec:eval:accuracy}, followed by the discussion of the required training time in \autoref{sec:eval:training-time}. We go on to discuss stability in \autoref{sec:eval:stability}, and conclude the chapter in \autoref{sec:eval:discussion}.

\section{Accuracy}\label{sec:eval:accuracy}
The accuracy on unseen data is a good indicator for how well a model predicts.
It is the share of correct predictions that match their target labels among the number of total predictions:

$$\frac{n_{correct}}{n_{total}} $$

In the following paragraphs, we present the maximum accuracies obtained on the validation set of each log.
For each log, we show a plot of the different model accuracies, grouped by batching strategy.
The presentation of the logs is ordered by process complexity, similar to \autoref{tab:dataset-characteristics}.
We end the section with a verdict of the observations.\\

% individual strategy
\paragraph{Accuracy on HelpDesk}
\autoref{fig:max-accuracies-helpdesk} displays the different accuracies on the validation set from the HelpDesk log.
With the individual, grouping strategies, all four models score above $0.80$.
As expected, the windowing strategy impacts accuracy significantly, and all accuracies see large degradations.
The SP2 model is impacted the least by the windowing strategy, presumably because its SP2 features capture the history that is cut away.
While the SCH, SP2 and PFS models do not show strong reactions to changes in the first three batching strategies, the EVM model fails with the padding strategy.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/helpdesk/accuracies.pdf}
    \caption{Best accuracies on the validation set of HelpDesk}
    \label{fig:max-accuracies-helpdesk}
\end{figure}
\FloatBarrier

\paragraph{Accuracy on BPIC12}
The process contained in BPIC12 is more complicated than the one captured in the HelpDesk log.
The accuracy measurements on it are visualized in \autoref{fig:max-accuracies-bpic2012}.
Again, all four models exhibit very similar accuracies for the individual and grouping batching strategies, where all score above $0.75$.
On the padding strategy, the EVM model barely reaches an accuracy above $0$, while the others score above $0.8$.
On all of the three strategies that supply the complete trace to the model, the accuracies are very similar.
The windowing strategy leads to more significant differences between the accuracies.
Here, the SP2 models outperform all other models.
The worst accuracy is shown by the EVM model on all four strategies, although the difference to the next-best accuracy is small.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2012/accuracies.pdf}
    \caption{Best accuracies on the validation set of BPIC12}
    \label{fig:max-accuracies-bpic2012}
\end{figure}
\FloatBarrier

\paragraph{Accuracy on BPIC15}
The accuracies obtained on the validation sets of the BPIC15 logs are visualized in \autoref{fig:max-accuracies-bpic2015-1} to \autoref{fig:max-accuracies-bpic2015-5}.
The results are very similar, so we describe the measurements in a grouping fashion per strategy and highlight common themes:

First, the accuracies of all four models are very similar across the individual and grouping strategies.
The biggest difference in accuracy is $0.1$.

Second, the SCH model always outperforms the other models slightly on the individual strategy.

Third, on the padding strategy, the SP2 model always is the most accurate.
On the same strategy, the EVM model fails.

Fourth, the SP2 model gives the highest accuracies on the grouping, padding and windowing strategies.
On the windowing strategy, its accuracies are the highest of all models by up to $0.2$.
The highest accuracies across all strategies are produced by the SP2 model on any BPIC15 dataset.

Fifth, the EVM model accuracy suffers the most from the windowing strategy.
It is a general rule that, on any BPIC15 dataset, the EVM model performs second-best on the individual strategy, while it ranks last on any other strategy.
Nonetheless, the differences to the next-best accuracy are always less than $0.2$, except for when the padding strategy is in use.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_1/accuracies.pdf}
    \caption{Best accuracies on the validation set of BPIC15-1}
    \label{fig:max-accuracies-bpic2015-1}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_2/accuracies.pdf}
    \caption{Best accuracies on the validation set of BPIC15-2}
    \label{fig:max-accuracies-bpic2015-2}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_3/accuracies.pdf}
    \caption{Best accuracies on the validation set of BPIC15-3}
    \label{fig:max-accuracies-bpic2015-3}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_4/accuracies.pdf}
    \caption{Best accuracies on the validation set of BPIC15-4}
    \label{fig:max-accuracies-bpic2015-4}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_5/accuracies.pdf}
    \caption{Best accuracies on the validation set of BPIC15-5}
    \label{fig:max-accuracies-bpic2015-5}
\end{figure}
\FloatBarrier

\paragraph{Accuracy on BPIC11}
BPIC11 captures the most complex process.
The validation accuracies that we obtained for this log are depicted in \autoref{fig:max-accuracies-bpic2011}.

With the individual strategy, the EVM, SCH and PFS models score above $0.6$.
The SP2 model reaches $0.4$.

In the case of the grouping strategy, the accuracies of all four models are very similar between $0.65$ and $0.7$.

The padding strategy sees another failure of the EVM model, while the other three score just above $0.6$.

The windowing strategy pushes the accuracies below $0.5$, with the SP2 model scoring exactly $0.5$.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2011/accuracies.pdf}
    \caption{Best accuracies on the validation set of BPIC11}
    \label{fig:max-accuracies-bpic2011}
\end{figure}

\paragraph{Verdict on accuracy}
Looking at the accuracy across all datasets, we detected five commonalities:

First, the batching strategies that supply the complete trace to the model lead to very similar results.

Second, the grouping strategy benefits high accuracies.
The mean of the four model accuracies on different batching strategies shows this in \autoref{tab:strategy-top-accuracies}.
The mean value is highest with the grouping strategy in six out of eight cases.
On BPIC12 however, the grouping strategy delivers relatively low accuracies.

Third, \autoref{tab:strategy-top-accuracies} on mean accuracies also reveals that the prediction accuracy goes down with increasing complexity of the process.
The models are most accurate on the HelpDesk log, and become more inaccurate toward BPIC11.

Fourth, the windowing strategy always results in underperforming models.
Nonetheless, the SP2 model copes with this strategy in the most efficient way, almost reaching maximum accuracies on BPIC15.

Fifth, the EVM model frequently was the most inaccurate, although with small differences to the next-best measurement.

This concludes the presentation of the accuracy results.
We continue with the presentation of the timing measurements in the following section.

\begin{table}
\centering
\begin{tabular}{p{3cm}rrrr}
\textbf{Strategy / Dataset}  &  \textbf{Individual} &  \textbf{Grouping} &   \textbf{Padding} &  \textbf{Windowing}\\
\midrule
\textbf{HelpDesk} &       0.854 &    0.844 &   0.625 &     0.666 \\
\textbf{BPIC12  } &       0.839 &    0.766 &   0.627 &     0.719 \\
\textbf{BPIC15-4} &       0.648 &    0.660 &   0.484 &     0.551 \\
\textbf{BPIC15-3} &       0.657 &    0.670 &   0.491 &     0.553 \\
\textbf{BPIC15-5} &       0.516 &    0.518 &   0.482 &     0.423 \\
\textbf{BPIC15-1} &       0.625 &    0.655 &   0.479 &     0.529 \\
\textbf{BPIC15-2} &       0.583 &    0.565 &   0.436 &     0.473 \\
\textbf{BPIC11  } &       0.598 &    0.674 &   0.474 &     0.473 \\
\end{tabular}
\caption[Mean accuracies per batching strategy]{Accuracy means for all models across a batching strategy. The grouping strategy is generally the highest. The rows are sorted by process complexity.}
\label{tab:strategy-top-accuracies}
\end{table}


\section{Training Time}\label{sec:eval:training-time}
The training time that a model requires for an epoch is important to gauge the efficiency of its training process.
As in the previous section, we discuss the measurements per log and finish with a verdict.
For each log, a plot is shown that presents the mean epoch training time per model, grouped by batching strategy.
The plots do not share the same scale on the y-axis.
During the following paragraphs, it is important to keep in mind that the batch size has a direct effect on the training time since it corresponds to the number of weight adjustments that need to be calculated.

\paragraph{Training times on HelpDesk}
How long training an epoch took with a model and a particular strategy on the HelpDesk log is shown in \autoref{fig:helpdesk-training-timings}.
Training with the grouping strategy takes the least amount of time and the most by far with the individual strategy.
The timings are very similar for all models on the same strategy with less than 10s difference.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/helpdesk/train_timings.pdf}
    \caption{Training times measured on HelpDesk}
    \label{fig:helpdesk-training-timings}
\end{figure}

\paragraph{Training times on BPIC12}
\autoref{fig:BPIC12-training-timings} depicts the epoch training times for BPIC12.
As for the HelpDesk log, the timings for models on the same strategy are very similar.
By far, they are the highest for each model on the individual strategy at approximately 10 minutes.
The other strategies take significantly less time.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2012/train_timings.pdf}
    \caption{Training times measured on BPIC12}
    \label{fig:BPIC12-training-timings}
\end{figure}

\paragraph{Training times on BPIC15}
The training timings taken during the model training on the BPIC15 log are shown in \autoref{fig:BPIC15-1-training-timings} to \autoref{fig:BPIC15-5-training-timings}.
The measurements are very similar across the five datasets in the log, which is why we explain them in a collective fashion.

The SCH, SP2 and PFS models always need a very similar amount of training time per epoch.
Training the EVM models often needs less time than the others, in some cases even less than half.
Across all five datasets, it is also possible to see that the individual strategy takes by far the most time.
Significantly less time is needed by the padding strategy, although it still ranks third.
The grouping strategy needs a little less time, and the windowing strategy trains fastest by far.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_1/train_timings.pdf}
    \caption{Training times measured on BPIC15-1}
    \label{fig:BPIC15-1-training-timings}
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_2/train_timings.pdf}
    \caption{Training times measured on BPIC15-2}
    \label{fig:BPIC15-2-training-timings}
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_3/train_timings.pdf}
    \caption{Training times measured on BPIC15-3}
    \label{fig:BPIC15-3-training-timings}
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_4/train_timings.pdf}
    \caption{Training times measured on BPIC15-4}
    \label{fig:BPIC15-4-training-timings}
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_5/train_timings.pdf}
    \caption{Training times measured on BPIC15-5}
    \label{fig:BPIC15-5-training-timings}
\end{figure}
\FloatBarrier

\paragraph{Training times on BPIC11}
\autoref{fig:BPIC11-training-timings} shows the training times measured on BPIC11.
On this plot, it is visible that the EVM model takes approximately half the training time of the other models.
As before, the other models require about the same time for an epoch for the same batching strategy.
The individual strategy causes the longest epochs.
The grouping strategy speeds up training by approximately a minute.
The padding strategy incurs a big drop in training times of around 6 minutes, and the windowing strategy is again the fastest.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2011/train_timings.pdf}
    \caption{Training times measured on BPIC11}
    \label{fig:BPIC11-training-timings}
\end{figure}

\paragraph{Verdict on training times}
Across all datasets, we gathered the following observations.
The individual strategy leads to the longest training times overall.
This is not surprising, as \autoref{tab:dataset-characteristics} on page \pageref{tab:dataset-characteristics} reveals that it creates hundreds of batches per epoch - more than with any other strategy.
Each batch incurs a weight adjustment, which leads to additional required computing time.

The grouping strategy leads to significant reductions in training times, depending on how diverse the trace lengths are. \autoref{tab:dataset-characteristics} also explains why the reduction is not as prominent with BPIC11: It exhibits up to 1814 different trace lengths, while the other datasets only exhibit $5\%$ to $10\%$ as many.

With the padding strategy, it is possible to overcome the limitations imposed by trace lengths and construct batches out of any number of traces. This makes it easier to optimize the batch size, which can lead to faster training times.

Fastest of all is the windowing strategy.
It produces a constant number of timesteps per sample and allows for arbitrary batch sizes.
The short samples additionally reduce the amount of calculation that needs to be done.

On each strategy, the SCH, SP2 and PFS models take approximately the same time to train.
The EVM model trains up to $50\%$ faster, depending on the dataset.
\FloatBarrier

\section{Stability}\label{sec:eval:stability}
As we stressed in the introduction, the stability of the prediction accuracy along the progress of a case can strongly impact the level of trust that users put into a model~\cite{metzger2015}.

In this section, we show the stability plots produced on each log, one for each batching strategy.
We discuss the logs in the order of increasing process complexity and finish with a verdict.
For space reasons, we only present the most informative graph per log in this section, and enclose the others in \autoref{appendix:evaluation-measurements}.

Each figure for a batching strategy and a dataset contains four curves, one for each model.
The curves show the model accuracy along the progression of all traces inside the validation set in steps of $5\%$.
The stability on the HelpDesk log was calculated in steps of $10\%$, as most cases in it are only 13 steps long.
The following paragraphs explain the curves in further detail.

\paragraph{Stability on HelpDesk}
On the HelpDesk log, it can be seen again that the batching strategies have a substantial impact on the accuracy in different stages.

\autoref{fig:helpdesk-individual-stability} shows the stability of each model with the individual strategy.
The four curves start around $0.8$ and linearly rise to $0.85$ until $60\%$ progress.
At the $60\%$ mark, all curves show a small correction, before going up toward an accuracy of $1$ at the end.

\autoref{fig:helpdesk-grouped-stability}, \autoref{fig:helpdesk-padded-stability}, and \autoref{fig:helpdesk-windowed-stability} show the stability on the other strategies.
With the grouping strategy, the curves are very similar, except for the EVM accuracy.
Its curve starts around $0.8$ and accelerates towards $0.5$ at $90\%$ progress.
Then, it shoots up to $1$ at the end.

The padding strategy makes all four models very unstable.
After the SCH, SP2 and PFS curves start around $0.8$, the curves see degradations and fluctuation around the $50\%$ mark, with the PFS model completely failing at $0$ accuracy at the end.
The EVM accuracy curve does not move above $0$.

The windowing strategy makes the curves very jittery.
All curves start at 0 accuracy and stay below $0.5$ accuracy until $60\%$ progress.
At this point, the curves shoot up above $0.8$ and finish at an accuracy of $1$.
Only the PFS curve finishes at $0.5$.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/helpdesk/individual_stability.pdf}
    \caption{Model stability, individual strategy, HelpDesk}
    \label{fig:helpdesk-individual-stability}
\end{figure}
\FloatBarrier

\paragraph{Stability on BPIC12}
The stability of the predictions on the BPIC12 log is discussed in this paragraph.
\autoref{fig:bpic12-individual-stability} shows the stability of each model with the individual strategy on BPIC12.
The four curves start around $0.8$ and immediately drop by $0.2$.
The SCH, SP2 and PFS curves swiftly recover by $15\%$ and rise to $0.9$ at $35\%$ progress
The EVM curve recovers slower from the initial drop, rejoining the others at $20\%$.
From $35\%$ onwards, all curves slowly sink toward $0.7$ at $85\%$ and rise sharply toward $1$ at the end.

\autoref{fig:bpic12-grouped-stability} shows the stability of the grouping strategy.
The trajectories of all four curves are mostly as described for the individual strategy.
A small, but notable difference is that the curves of the SCH, SP2 and PFS models are much closer together, and the EVM curve consistently set lower.

In \autoref{fig:bpic12-padded-stability}, the stability for the padding strategy is shown.
The curve trajectories are again very similar to the two aforementioned stability plots, but the overall accuracies are lower.
The EVM model reacts strongly to the padding strategy.
Its accuracy curve now starts at $0$ accuracy, shortly reaches $0.15$ around $80\%$ and then drops back to $0$ accuracy at the end.
The other three curves show a stronger depression around the $75\%$ mark, dropping down to an accuracy of $0.5$, before recovering to $1$ at the end.
The SCH curve even drops to $0.4$.

Finally, the stability plot of the windowing strategy in \autoref{fig:bpic12-windowed-stability} reveals entirely new curve trajectories.
The SP2 curve starts at $0.8$, drops by $0.1$, and recovers back to $0.9$ until $50\%$ progress.
At this point, there is a sharp decline to $0.7$.
The curve recovers just as sharply, and slowly degrades to $0.6$, and shoots up to $1$ in the end.
The EVM, SCH and PFS curves show a similar trajectory as the SP2 curve, albeit less accurate by around $0.150$.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2012/individual_stability.pdf}
    \caption{Model stability, individual strategy, BPIC12}
    \label{fig:bpic12-individual-stability}
\end{figure}
\FloatBarrier

\paragraph{Stability on BPIC15}
The BPIC15 log with its five datasets is discussed in this paragraph.
The figures \autoref{fig:bpic15-5-individual-stability}, \autoref{fig:bpic15-5-grouped-stability} and \autoref{fig:bpic15-5-windowed-stability} show the stabilities for the individual, grouping and windowing strategies on BPIC15-5.
In the appendix, \autoref{fig:bpic15-1-individual-stability} to \autoref{fig:bpic15-5-padded-stability} illustrate the remaining curves for the remainder of the log.
The curves across BPIC15-1 to BPIC15-5 are very similar within a specific strategy, which is why we give a general description of the observations for each strategy in the following.

In the stability plot of the individual strategy in \autoref{fig:bpic15-5-individual-stability}, all four curves follow the same trend.
They start around $0.7$ and sink to $0.6$ at $15\%$.
At this point, the PFS curve is $0.15$ below the rest and stays at this distance until the end.
After $15\%$, the curves rise back up to $0.7$ at $35\%$.
From there on, they descend to $0.5$ at $95\%$.
Then, all three curves see an increase of $0.2$, with the SP2 curve ending at $0.75$.
The SCH curve ends at $0.7$, and the PFS curve at $0.6$.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_5/individual_stability.pdf}
    \caption{Model stability, individual strategy, BPIC15-5}
    \label{fig:bpic15-5-individual-stability}
\end{figure}

The stability plot for the grouping strategy in \autoref{fig:bpic15-5-grouped-stability} shows very similar curve trajectories.
In contrast to the individual strategy, the four curves are a lot closer together, and the EVM curve has moved to the lower end.
This is visible on the other BPIC15 datasets, too.

The plots for the padding strategy show steadily declining curves for SCH, SP2 and PFS models from $0.7$ at the beginning towards an accuracy of $0.4$ around $90\%$ progress.
Then, the curves jump up by $0.2$ to $0.3$ to finish around $0.7$.
The EVM curve does not surface above $0$.
For BPIC15-1 and BPIC15-4, the SCH curve also dips down after the start.
On BPIC15-1, the PFS curve also dips down, leaving the SP2 curve as the sole remainder on the steadily declining trajectory described.

The plots for the windowing strategy in \autoref{fig:bpic15-5-windowed-stability} paint a simple picture.
The EVM, SCH and PFS curves start around $0.6$, and descend toward $0.3$, spiking up toward $0.5$ at the end.
The SP2 curve hovers above, starting around $0.8$, and linearly descends to $0.55$ until $90\%$ progress.
Then, it also jumps up by $0.3$ at the end of the plot.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_5/grouped_stability.pdf}
    \caption{Model stability, grouping strategy, BPIC15-5}
    \label{fig:bpic15-5-grouped-stability}
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2015_5/windowed_stability.pdf}
    \caption{Model stability, windowing strategy, BPIC15-5}
    \label{fig:bpic15-5-windowed-stability}
\end{figure}
\newpage

\paragraph{Stability on BPIC11}
The most informative plot from the BPIC11 stability measurements is shown in \autoref{fig:bpic11-grouped-stability} for the grouping strategy.
It illustrates how the accuracy is relatively constant for the EVM, SP2 and PFS models around $0.7$.
Also, the plot shows well how close the curves of the three models are.
All curves drop slightly in the beginning and then stay constant until the end, where there is another small drop.
The SP2 model accuracy drops heavier in the beginning and stays constant around $0.65$.

Little do the curves change for the individual strategy.
Only two minor differences surface: The SCH and PFS curves are further apart, and the SP2 curve drops asymptotically from $0.65$ in the beginning to $0.1$ at the end.

With the padding strategy, the SCH, SP2 and PFS curves start around $0.75$, drop to approximately $0.55$ at $55\%$ and stay constant until the end of the process.
There is no accuracy drop in the end.
The EVM curve stays level at $0$ accuracy.

The windowing strategy makes the initial drop from all four models more pronounced from $0.75$ in the beginning to just above $0.4$ from $35\%$ onwards.
The SP2 model accuracy is approximately $0.1$ higher than the others until sinking to their level $80\%$
From there on, the curves stay constant.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/bpic2011/grouped_stability.pdf}
    \caption{Stability curves for grouping strategy on BPIC11}
    \label{fig:bpic11-grouped-stability}
\end{figure}

\paragraph{Verdict on stability}
In our verdict on the stability measurements across all datasets, we focus on five key observations.

First, we observed that many accuracy curves started with a drop in accuracy.
Similarly, accuracy spiked up towards the end of the process, like on the BPIC12 log.
The BPIC12 log contains traces from a process that covers a loan application.
On this log, the accuracies taper off until $85\%$ progress, when they spike up again.

\begin{figure}[!htb]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=1.25\textwidth]{gfx/bpic12-mined-process.png}
    \end{adjustbox}
    \caption[A mined process model from BPIC12]{A mined process model from BPIC12. Illustration taken from~\cite{adriansyah2012mining}}
    \label{fig:bpic12-mined-process}
\end{figure}

\autoref{fig:bpic12-mined-process} reveals a process model that was mined from BPIC12.
Seeing the model in context with the stability curve reveals that the accuracy drops and spikes could be related to the varying degrees of complexity in different stages of the process.
Every application begins in the same way, i.e., variability in the first stages of the process is low, which might cause accuracy to be high.
As the process progresses, the variability increases, which correlates with the accuracy decline until $80\%$ progress.
The processes in BPIC12 always end either on approval or dismissal of the application, boosting accuracy from the moment that no more splits have to be anticipated.
This could hint at a connection between prediction accuracy and complexity of the stage of a process.

Second, we realized that the grouping strategy did not only contribute to high overall results but brought curves closer together for the four models.
This might be a symptom of a harmonizing effect of the strategy.
We visualized this effect in \autoref{fig:grouping-accuracy-harmonization}.
The heatmap shows the standard deviation among the best accuracies of the SCH, SP2 and PFS models for each dataset and strategy.
It is clear that the grouping strategy leads to the lowest standard deviations.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{gfx/grouping-accuracy-harmonization.pdf}
    \caption[Batching strategy harmonizes top accuracies]{The grouping strategy often reduces the standard deviation between the highest accuracies of the models}
    \label{fig:grouping-accuracy-harmonization}
\end{figure}

Third, within a batching strategy, all models produce accuracy curves that follow the same trend.
On the padding strategy, the EVM model consistently breaks down.

Fourth, the windowing strategy incurs very unstable curves.
While this statement seems to be especially true for the longer processes in BPIC11 and BPIC15, the windowing strategy seems to have less of an impact with shorter processes as evidenced with BPIC12.

Fifth, the SP2 model best handles the missing history on the windowing strategy.
Its curves are the highest among the windowing strategy measurements on any dataset.
Also, the SP2 accuracy curves on the windowing strategy are among those with the smallest fluctuations.\\

We connect these observations to the other findings from the other verdicts in the following section.

\section{Discussion}\label{sec:eval:discussion}
In this section, we connect the observations made in the previous sections on accuracy, timing, and stability of the models.
Furthermore, we discuss resulting learnings and potential reasons.\\

The EVM model consistently underperforms, although it trains faster.
Furthermore, it does not seem to work at all with the padding strategy.
The SCH model works better, although both architectures are very similar.
A fundamental difference between the EVM and SCH models is an embedding layer, which we suspect to be the cause for the lower accuracy.
We believe that the embedding layer requires more data points per target class than currently included to perform well.
In the HelpDesk log, there is a tiny number of classes and a large number of traces, which could be a reason for the EVM model scoring above $0.85$.
Also, we noted that the EVM model produced relatively better accuracies with the individual strategy than on the others.
This might be due to more frequent adjustments of the word embedding.\\

We find that the grouping strategy frequently delivers the best results in terms of speed and accuracy.
Nonetheless, there is potential for optimization, as the relatively low accuracy on BPIC12 suggests.
The reason for this accuracy can be found in \autoref{tab:dataset-characteristics} and \autoref{tab:batch-sizes} in \autoref{chap:methodology}.
BPIC12 has the highest number of traces, and a relatively even distribution of lengths, resulting in a small standard deviation.
This makes the strategy place too many traces into a single batch, causing lost optimization opportunities.
With this in mind, the grouping strategy should be enhanced to split batches if they exceed a certain size threshold.\\

We were also able to confirm Klinkm√ºller et al. and their statement that the windowing strategy leads to unstable results~\cite{klinkmuller2018reliablemonitoring}.
While it may be a performant strategy to use for time-series prediction, it does not work very well for predicting the future of a single case.\\

Furthermore, we expected the models to become more accurate toward the end of the process.
This did not turn out to be the case, but instead, we saw a connection of process complexity and accuracy.
This connection was visible both in absolute accuracy and in prediction stability measurements.
\autoref{tab:strategy-top-accuracies} on page \pageref{tab:strategy-top-accuracies} shows that the mean accuracy of the three top-performing models on each dataset goes down as the process complexity goes up.
In the stability measurements, it was possible to see that the models gave very accurate predictions in phases of low variability.
We showed in the preceding section that process phases with few decision points coincide with phases of higher accuracy.

It is worth reflecting on this point.
If a machine learning model is incapable of understanding the choices made in certain phases of a process, it could either be a data problem or a process problem.
In the latter case, a potential reason could be that decisions in phases of low accuracy are not standardized or overly complicated.
Using the stability information in this way could create a feedback loop for process optimization.\\

After discussing potential reasons for our findings and consequences thereof, we compare our accuracy measurements to other works on the same datasets.

\section{Comparison to Other Works}
We end the evaluation with a comparison of our best results with publications that also based their results on the BPIC12 and HelpDesk logs.
The comparison is presented in \autoref{tab:accuracy-comparison}.

We want to stress that while Evermann et al.~\cite{evermann2016} were able to obtain an accuracy of $0.768$ for predicting the next event on BPIC12, they did not focus on specific cases.
They focused on the whole event stream, and thus did not plan to predict the future of a single case.

B√∂hmer et al. and Tax et al. worked on next-activity prediction for a particular case.
B√∂hmer et al. used a method based on probability distributions and obtained $0.77$ on both logs.
Tax et al. made use of neural networks, and obtained $0.71$ on the BPIC12 log and $0.76$ on the HelpDesk log.

The comparison table shows that all four models deliver higher accuracies on both logs.
The PFS model surpasses the accuracies in the other works by $0.09$ or more on the HelpDesk log.
On the BPIC12 log, the SCH model outperforms the other works by $0.08$ or more.
We obtained these accuracies with the individual strategy.\\

\begin{table}
\centering
\begin{tabular}{lrr}
\textbf{Model}  &  \textbf{HelpDesk} &  \textbf{BPIC12} \\
\midrule
\textbf{Evermann et al.~\cite{evermann2016}} & - & $0.768$\\
\textbf{B√∂hmer et al.~\cite{boehmer2018probability}  } & $0.77$ & $0.77$ \\
\textbf{Tax et al.~\cite{tax2017}} & $0.71$ & $0.76$\\
\hline
\textbf{PFS} & $0.862$ & $0.848$ \\
\textbf{SCH} & $0.854$ & $0.853$ \\
\textbf{SP2} & $0.848$ & $0.850$ \\
\textbf{EVM} & $0.853$ & $0.802$ \\
\end{tabular}
\caption{Accuracy comparison to published numbers}
\label{tab:accuracy-comparison}
\end{table}

In this evaluation, we discussed the accuracy, training time and accuracy stability of the models on the individual logs.
We determined that the grouping strategy delivers the most promising results, but can still be optimized.
Furthermore, we noted that the complexity of the traces in the log has a significant impact on stability.
Depending on the amount of variability in different stages of the process, model accuracy varies dramatically.
We could confirm that our models outperform recently published approaches.
The next chapter describes ways to improve and continue our research and concludes the thesis with a summary of the results.
