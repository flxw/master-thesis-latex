\chapter{Evaluation}\label{chap:evaluation}


\section{Implementation}
Model architecture diagrams

Baseline: Evermann
Data: Schönig
SP2/PFS: Shibata on Business processes

Keras, tuning with hyperas, FSOC, Anaconda environment,

one-hot encoding for certain columns since no ordinal relationship present

drop lifecycle:transition
activity code correlates to 
Keras, FSOC, Docker, Anaconda

\section{Data transformation}
Which columns were dropped and why?

\section{Test setup}
Evermann network architecture, Schönig network architecture, my network architecture

Dropping the embedding layer if not embedding is present

\section{Evaluation criteria}
Three metrics of the four aforementioned implementations shall be the focus of the evaluation of the results:
\begin{enumerate}
    \item\textbf{Accuracy} - I.e. how many next-activity predictions were correct in total.
    \item\textbf{Earliness} - I.e. whether and how early in the total trace the prediction accuracy begins to stabilize \cite{francescomarino2015}.
    \item\textbf{Feature importance} - I.e. whether network-internal metrics indicate that an added feature is valuable for making predictions.
\end{enumerate}

\todo[inline]{MAKE THIS FIGURE!}
\begin{figure}
    \centering
    % \includegraphics{}
    \caption{figure of the four implementations side by side}
    \label{fig:my_label}
\end{figure}

Accuracy and earliness

Does accuracy rise with increasing trace length?

\section{Evaluation results}
Feature Importance score extraction

\section{Result discussion}
Is the hassle worth it?
Increase in memory, training time...?
Compare with Evermanns etc results